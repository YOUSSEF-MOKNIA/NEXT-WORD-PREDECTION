{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d5fb432",
   "metadata": {},
   "source": [
    "## READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fd08d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data text file\n",
    "with open (\"./Dataset/sherlock-holm.es_stories_plain-text_advs.txt\", 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51144bca",
   "metadata": {},
   "source": [
    "## TOKENIZE THE TEXT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "156bfe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# Fit the tokenizer on the text to create a vocabulary\n",
    "tokenizer.fit_on_texts([text])\n",
    "# Get the total number of unique words\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0eb62301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8200"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed99de2",
   "metadata": {},
   "source": [
    "## FORMING N-GRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9f0e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []  # Initialize an empty list to store input sequences\n",
    "\n",
    "# Split the text into lines using the newline character as a delimiter\n",
    "for line in text.split('\\n'):  \n",
    "    # Convert the line of text into a sequence of tokens based on the tokenizer's vocabulary\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]  \n",
    "\n",
    "    # Iterate over the token list to create n-gram sequences\n",
    "    for i in range(1, len(token_list)):  \n",
    "        # Extract a subsequence (n-gram) from the beginning of the token list up to the current index i+1\n",
    "        n_gram_sequence = token_list[:i+1]  \n",
    "        # Append the n-gram sequence to the list of input sequences\n",
    "        input_sequences.append(n_gram_sequence)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509801e5",
   "metadata": {},
   "source": [
    "## PAD THE INPUT SEQUENCES TO HAVE EQUAL LENGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad42302a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Calculate the length of the longest sequence in the input_sequences list\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "\n",
    "# Pad all sequences in input_sequences to have the same length as the longest sequence\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6c0ef3",
   "metadata": {},
   "source": [
    "## SPLIT THE SEQUENCES INTO INPUT AND OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14c619b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign X the input sequences without the last token in each sequence\n",
    "X = input_sequences[:, :-1]\n",
    "# Assign y the last token of each sequence as the target output\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7aa75",
   "metadata": {},
   "source": [
    "- **[:, :-1]:** This notation means \"select all rows, and all columns except the last one.\n",
    "- **[:, -1]:** This notation means \"select all rows, and only the last column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46889f8",
   "metadata": {},
   "source": [
    "## CONVERT THE OUTPUT TO ONE-HOT ENCODE VECTORS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22b6a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert the output array 'y' to one-hot encoded vectors\n",
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c459b39",
   "metadata": {},
   "source": [
    "## BUILD THE NEURAL NETWORK ARCHI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c5a31d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 17, 100)           820000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 150)               150600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8200)              1238200   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,208,800\n",
      "Trainable params: 2,208,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Embedding(total_words, 100, input_length=max_sequence_len-1),\n",
    "        LSTM(150),\n",
    "        Dense(total_words, activation='softmax'),\n",
    "    ]\n",
    ")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a8721c",
   "metadata": {},
   "source": [
    "## COMPILE AND TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "239ba702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3010/3010 [==============================] - 152s 50ms/step - loss: 6.2248 - accuracy: 0.0780\n",
      "Epoch 2/100\n",
      "3010/3010 [==============================] - 126s 42ms/step - loss: 5.4917 - accuracy: 0.1264\n",
      "Epoch 3/100\n",
      "3010/3010 [==============================] - 138s 46ms/step - loss: 5.1127 - accuracy: 0.1484\n",
      "Epoch 4/100\n",
      "3010/3010 [==============================] - 126s 42ms/step - loss: 4.7900 - accuracy: 0.1669\n",
      "Epoch 5/100\n",
      "3010/3010 [==============================] - 129s 43ms/step - loss: 4.4891 - accuracy: 0.1830\n",
      "Epoch 6/100\n",
      "3010/3010 [==============================] - 101s 34ms/step - loss: 4.2032 - accuracy: 0.2034\n",
      "Epoch 7/100\n",
      "3010/3010 [==============================] - 104s 35ms/step - loss: 3.9339 - accuracy: 0.2268\n",
      "Epoch 8/100\n",
      "3010/3010 [==============================] - 104s 35ms/step - loss: 3.6804 - accuracy: 0.2570\n",
      "Epoch 9/100\n",
      "3010/3010 [==============================] - 107s 35ms/step - loss: 3.4447 - accuracy: 0.2886\n",
      "Epoch 10/100\n",
      "3010/3010 [==============================] - 118s 39ms/step - loss: 3.2251 - accuracy: 0.3231\n",
      "Epoch 11/100\n",
      "3010/3010 [==============================] - 116s 39ms/step - loss: 3.0197 - accuracy: 0.3566\n",
      "Epoch 12/100\n",
      "3010/3010 [==============================] - 114s 38ms/step - loss: 2.8297 - accuracy: 0.3897\n",
      "Epoch 13/100\n",
      "3010/3010 [==============================] - 113s 37ms/step - loss: 2.6535 - accuracy: 0.4238\n",
      "Epoch 14/100\n",
      "3010/3010 [==============================] - 115s 38ms/step - loss: 2.4921 - accuracy: 0.4534\n",
      "Epoch 15/100\n",
      "3010/3010 [==============================] - 119s 40ms/step - loss: 2.3414 - accuracy: 0.4839\n",
      "Epoch 16/100\n",
      "3010/3010 [==============================] - 120s 40ms/step - loss: 2.2026 - accuracy: 0.5121\n",
      "Epoch 17/100\n",
      "3010/3010 [==============================] - 117s 39ms/step - loss: 2.0769 - accuracy: 0.5390\n",
      "Epoch 18/100\n",
      "3010/3010 [==============================] - 114s 38ms/step - loss: 1.9588 - accuracy: 0.5628\n",
      "Epoch 19/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 1.8512 - accuracy: 0.5863\n",
      "Epoch 20/100\n",
      "3010/3010 [==============================] - 107s 35ms/step - loss: 1.7504 - accuracy: 0.6099\n",
      "Epoch 21/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 1.6575 - accuracy: 0.6282\n",
      "Epoch 22/100\n",
      "3010/3010 [==============================] - 109s 36ms/step - loss: 1.5743 - accuracy: 0.6456\n",
      "Epoch 23/100\n",
      "3010/3010 [==============================] - 110s 37ms/step - loss: 1.4968 - accuracy: 0.6636\n",
      "Epoch 24/100\n",
      "3010/3010 [==============================] - 110s 37ms/step - loss: 1.4242 - accuracy: 0.6808\n",
      "Epoch 25/100\n",
      "3010/3010 [==============================] - 109s 36ms/step - loss: 1.3589 - accuracy: 0.6950\n",
      "Epoch 26/100\n",
      "3010/3010 [==============================] - 109s 36ms/step - loss: 1.2980 - accuracy: 0.7065\n",
      "Epoch 27/100\n",
      "3010/3010 [==============================] - 112s 37ms/step - loss: 1.2410 - accuracy: 0.7194\n",
      "Epoch 28/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 1.1859 - accuracy: 0.7317\n",
      "Epoch 29/100\n",
      "3010/3010 [==============================] - 105s 35ms/step - loss: 1.1406 - accuracy: 0.7419\n",
      "Epoch 30/100\n",
      "3010/3010 [==============================] - 105s 35ms/step - loss: 1.0964 - accuracy: 0.7520\n",
      "Epoch 31/100\n",
      "3010/3010 [==============================] - 105s 35ms/step - loss: 1.0533 - accuracy: 0.7618\n",
      "Epoch 32/100\n",
      "3010/3010 [==============================] - 104s 35ms/step - loss: 1.0161 - accuracy: 0.7690\n",
      "Epoch 33/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 0.9805 - accuracy: 0.7775\n",
      "Epoch 34/100\n",
      "3010/3010 [==============================] - 108s 36ms/step - loss: 0.9472 - accuracy: 0.7845\n",
      "Epoch 35/100\n",
      "3010/3010 [==============================] - 105s 35ms/step - loss: 0.9158 - accuracy: 0.7906\n",
      "Epoch 36/100\n",
      "3010/3010 [==============================] - 107s 35ms/step - loss: 0.8918 - accuracy: 0.7953\n",
      "Epoch 37/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 0.8610 - accuracy: 0.8022\n",
      "Epoch 38/100\n",
      "3010/3010 [==============================] - 109s 36ms/step - loss: 0.8407 - accuracy: 0.8059\n",
      "Epoch 39/100\n",
      "3010/3010 [==============================] - 108s 36ms/step - loss: 0.8161 - accuracy: 0.8113\n",
      "Epoch 40/100\n",
      "3010/3010 [==============================] - 108s 36ms/step - loss: 0.7961 - accuracy: 0.8161\n",
      "Epoch 41/100\n",
      "3010/3010 [==============================] - 109s 36ms/step - loss: 0.7770 - accuracy: 0.8198\n",
      "Epoch 42/100\n",
      "3010/3010 [==============================] - 109s 36ms/step - loss: 0.7616 - accuracy: 0.8225\n",
      "Epoch 43/100\n",
      "3010/3010 [==============================] - 107s 36ms/step - loss: 0.7414 - accuracy: 0.8266\n",
      "Epoch 44/100\n",
      "3010/3010 [==============================] - 105s 35ms/step - loss: 0.7270 - accuracy: 0.8302\n",
      "Epoch 45/100\n",
      "3010/3010 [==============================] - 107s 35ms/step - loss: 0.7148 - accuracy: 0.8329\n",
      "Epoch 46/100\n",
      "3010/3010 [==============================] - 108s 36ms/step - loss: 0.7000 - accuracy: 0.8362\n",
      "Epoch 47/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 0.6897 - accuracy: 0.8377\n",
      "Epoch 48/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 0.6779 - accuracy: 0.8394\n",
      "Epoch 49/100\n",
      "3010/3010 [==============================] - 111s 37ms/step - loss: 0.6678 - accuracy: 0.8413\n",
      "Epoch 50/100\n",
      "3010/3010 [==============================] - 109s 36ms/step - loss: 0.6552 - accuracy: 0.8449\n",
      "Epoch 51/100\n",
      "3010/3010 [==============================] - 110s 36ms/step - loss: 0.6494 - accuracy: 0.8446\n",
      "Epoch 52/100\n",
      "3010/3010 [==============================] - 110s 37ms/step - loss: 0.6429 - accuracy: 0.8463\n",
      "Epoch 53/100\n",
      "3010/3010 [==============================] - 111s 37ms/step - loss: 0.6321 - accuracy: 0.8484\n",
      "Epoch 54/100\n",
      "3010/3010 [==============================] - 112s 37ms/step - loss: 0.6207 - accuracy: 0.8509\n",
      "Epoch 55/100\n",
      "3010/3010 [==============================] - 107s 36ms/step - loss: 0.6193 - accuracy: 0.8501\n",
      "Epoch 56/100\n",
      "3010/3010 [==============================] - 107s 36ms/step - loss: 0.6066 - accuracy: 0.8548\n",
      "Epoch 57/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 0.6044 - accuracy: 0.8535\n",
      "Epoch 58/100\n",
      "3010/3010 [==============================] - 108s 36ms/step - loss: 0.6021 - accuracy: 0.8537\n",
      "Epoch 59/100\n",
      "3010/3010 [==============================] - 111s 37ms/step - loss: 0.5925 - accuracy: 0.8542\n",
      "Epoch 60/100\n",
      "3010/3010 [==============================] - 109s 36ms/step - loss: 0.5852 - accuracy: 0.8582\n",
      "Epoch 61/100\n",
      "3010/3010 [==============================] - 108s 36ms/step - loss: 0.5837 - accuracy: 0.8567\n",
      "Epoch 62/100\n",
      "3010/3010 [==============================] - 107s 35ms/step - loss: 0.5805 - accuracy: 0.8575\n",
      "Epoch 63/100\n",
      "3010/3010 [==============================] - 107s 36ms/step - loss: 0.5722 - accuracy: 0.8589\n",
      "Epoch 64/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 0.5682 - accuracy: 0.8603\n",
      "Epoch 65/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 0.5671 - accuracy: 0.8603\n",
      "Epoch 66/100\n",
      "3010/3010 [==============================] - 110s 37ms/step - loss: 0.5644 - accuracy: 0.8603\n",
      "Epoch 67/100\n",
      "3010/3010 [==============================] - 108s 36ms/step - loss: 0.5581 - accuracy: 0.8612\n",
      "Epoch 68/100\n",
      "3010/3010 [==============================] - 107s 35ms/step - loss: 0.5563 - accuracy: 0.8622\n",
      "Epoch 69/100\n",
      "3010/3010 [==============================] - 104s 34ms/step - loss: 0.5505 - accuracy: 0.8635\n",
      "Epoch 70/100\n",
      "3010/3010 [==============================] - 105s 35ms/step - loss: 0.5522 - accuracy: 0.8626\n",
      "Epoch 71/100\n",
      "3010/3010 [==============================] - 104s 34ms/step - loss: 0.5456 - accuracy: 0.8630\n",
      "Epoch 72/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 0.5454 - accuracy: 0.8622\n",
      "Epoch 73/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 0.5468 - accuracy: 0.8614\n",
      "Epoch 74/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 0.5377 - accuracy: 0.8645\n",
      "Epoch 75/100\n",
      "3010/3010 [==============================] - 105s 35ms/step - loss: 0.5405 - accuracy: 0.8633\n",
      "Epoch 76/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 0.5378 - accuracy: 0.8638\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3010/3010 [==============================] - 102s 34ms/step - loss: 0.5363 - accuracy: 0.8643\n",
      "Epoch 78/100\n",
      "3010/3010 [==============================] - 105s 35ms/step - loss: 0.5311 - accuracy: 0.8649\n",
      "Epoch 79/100\n",
      "3010/3010 [==============================] - 103s 34ms/step - loss: 0.5338 - accuracy: 0.8630\n",
      "Epoch 80/100\n",
      "3010/3010 [==============================] - 102s 34ms/step - loss: 0.5292 - accuracy: 0.8649\n",
      "Epoch 81/100\n",
      "3010/3010 [==============================] - 102s 34ms/step - loss: 0.5335 - accuracy: 0.8639\n",
      "Epoch 82/100\n",
      "3010/3010 [==============================] - 103s 34ms/step - loss: 0.5261 - accuracy: 0.8649\n",
      "Epoch 83/100\n",
      "3010/3010 [==============================] - 103s 34ms/step - loss: 0.5249 - accuracy: 0.8648\n",
      "Epoch 84/100\n",
      "3010/3010 [==============================] - 103s 34ms/step - loss: 0.5223 - accuracy: 0.8672\n",
      "Epoch 85/100\n",
      "3010/3010 [==============================] - 102s 34ms/step - loss: 0.5196 - accuracy: 0.8669\n",
      "Epoch 86/100\n",
      "3010/3010 [==============================] - 105s 35ms/step - loss: 0.5201 - accuracy: 0.8664\n",
      "Epoch 87/100\n",
      "3010/3010 [==============================] - 112s 37ms/step - loss: 0.5212 - accuracy: 0.8646\n",
      "Epoch 88/100\n",
      "3010/3010 [==============================] - 110s 36ms/step - loss: 0.5165 - accuracy: 0.8672\n",
      "Epoch 89/100\n",
      "3010/3010 [==============================] - 109s 36ms/step - loss: 0.5179 - accuracy: 0.8667\n",
      "Epoch 90/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 0.5158 - accuracy: 0.8672\n",
      "Epoch 91/100\n",
      "3010/3010 [==============================] - 109s 36ms/step - loss: 0.5197 - accuracy: 0.8646\n",
      "Epoch 92/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 0.5120 - accuracy: 0.8670\n",
      "Epoch 93/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 0.5168 - accuracy: 0.8653\n",
      "Epoch 94/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 0.5147 - accuracy: 0.8664\n",
      "Epoch 95/100\n",
      "3010/3010 [==============================] - 109s 36ms/step - loss: 0.5144 - accuracy: 0.8649\n",
      "Epoch 96/100\n",
      "3010/3010 [==============================] - 108s 36ms/step - loss: 0.5158 - accuracy: 0.8644\n",
      "Epoch 97/100\n",
      "3010/3010 [==============================] - 105s 35ms/step - loss: 0.5126 - accuracy: 0.8657\n",
      "Epoch 98/100\n",
      "3010/3010 [==============================] - 105s 35ms/step - loss: 0.5109 - accuracy: 0.8666\n",
      "Epoch 99/100\n",
      "3010/3010 [==============================] - 105s 35ms/step - loss: 0.5096 - accuracy: 0.8670\n",
      "Epoch 100/100\n",
      "3010/3010 [==============================] - 106s 35ms/step - loss: 0.5110 - accuracy: 0.8664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19b6eea68f0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model : prepares the model for training\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', # Loss function to measure how well the model is performing\n",
    "    optimizer='adam',                # Optimizer to update the model weights during training\n",
    "    metrics=['accuracy']             # Metric to evaluate the model's performance\n",
    ")\n",
    "\n",
    "# Train the model with the training data\n",
    "model.fit(\n",
    "    X,           # Input data (features)\n",
    "    y,           # Target data (labels)\n",
    "    epochs=100,  # Number of times to iterate over the entire dataset\n",
    "    verbose=1    # Verbosity mode, 1 means progress bar with logs for each epoch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "59ded041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "i am glad to hear\n"
     ]
    }
   ],
   "source": [
    "# Initial seed text to start the prediction\n",
    "seed_text = \"i am\"\n",
    "\n",
    "# Number of words to predict\n",
    "next_words = 3\n",
    "\n",
    "# Loop to generate the next words\n",
    "for _ in range(next_words):\n",
    "    # Convert the seed text into a sequence of tokens\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    \n",
    "    # Pad the sequence to match the input length of the model\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    \n",
    "    # Predict the next word's index\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    \n",
    "    # Find the word that corresponds to the predicted index\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    \n",
    "    # Add the predicted word to the seed text\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "# Print the final generated text\n",
    "print(seed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abdc190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
